---
title: AlexNet
categories:
 - CV
tags:
 - 新手村
 - CV
---

从论文[AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)摘要了一些重要的点，对卷积层，池化层等涉及的基础知识进行了补充。

<br />

<!--more-->

***

## 0. 补充学习

### a. 卷积层

  用卷积核(kernel,n*n matrix)在图片上从左至右，再从上到下按步长移动，每移动一次，将点乘结果sum，生成一个数据。

### b. 池化层
  在图像中矩形分割，取矩形中最大(或平均)值。

### c. 反向传播（待补充）

  全连接反向传播

![反向传播](/pictures/Backprop.jpeg)

  cnn反向传播更新算法

 ![cnn back](/pictures/Backprop_cnn.jpeg)

 ~~从入门到放弃~~

 在学习中

***

## 1. The Architecture

### a. ReLU(Rectified Linear Units) Nonlinearity

  A neuron’s output `f(x) = max(0, x)`  rather than sigmoid or tanh（speed upppp）

### b. Local Response Normalization(remaining)

  ReLUs don't require input normalization to prevent them from saturating. But local normalization scheme aids generalization.

![norm](/pictures/ReLU_normallization.png)

根据其他kernel，对当前层的输出结果做平滑处理，a(i,x,y)表示第i个内核计算(x,y)位置的ReLU非线性单元的输出

k = 2, n = 5, α = 10−4, and β = 0.75

N为总层数,其他为超参

### c. Overlapping Pooling

  A (max) pooling layer can be thought of as consisting of a grid of pooling units spaced s pixels apart, each summarizing a neighborhood of size z × z centered at the location of the pooling unit. 

  s = 2 and z = 3

### d. Overall Architecture

![overallarch](/pictures/ReLU_architecture.png)

  pool in layer 1,2,5  ; norm in layer 1,2 ; ReLU in every layer except the output

***

## 2. Reduce overfitting

### a.Data Augmentation

* extracting five 224 × 224 patches (the four corner patches and the center patch) as well as their horizontal
reflections.
* perform PCA on the set of RGB pixel

### b. Dropout

  Setting to zero the output of each hidden neuron with probability 0.5. Reduces complex co-adaptations of neurons。

***

## 3. Details of learning

 ![cnn back](/pictures/Backprop_cnn.jpeg)

  with a batch size of 128 examples, momentum of 0.9, and weight decay of 0.0005, where i is the iteration index, v is the momentum variable, e is the learning rate

## 4. 在MNIST上的实现

### a. 数据预处理

  黑白数字图像无法进行论文中的两种Data Augmentation，只进行了归一化处理。（0-225 → 0-1）

### b. 网络结构

  5个卷积层+3个全连接层,根据数据不同对具体参数进行了相应改动

* 第一层`28*28*1` 卷积核 16 `3*3*1` s=1 池化层 `3*3` stride=2
* 第二层`28*28*16` 卷积核 48 `3*3*16` s=2 池化层 `3*3` stride=2
* 第三层`13*13*48` 卷积核 96 `3*3*48` s=2
* 第四层`7*7*96` 卷积核 192 `3*3*96` s=1
* 第五层`7*7*192` 卷积核 192 `3*3*192` s=1 池化层 `3*3` stride=2
* 第六层 4800 dropout
* 第七层 2048 dropout
* 第八层 10
  
  前7层使用ReLU；1，2，5层池化；6，7层dropout
  
### c. 算法

  Adadelta(lr=0.3, rho=0.9, weight_decay=0.0005） + SGD

  超参：EPOCH = 2（太慢了）  BATCH_SIZE = 50  LR = 0.03

### d. 结果
  98.46

  ![cnn_result](/pictures/alex_cnn_result.png)

### e. 改善

  有空加上论文中给出的归一化公式与反向传播算法



