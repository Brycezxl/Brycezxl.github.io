---
title: Pytorch
tags:
 - 新手村
---

Pytorch一些基础的tensor，数学操作，以及神经网络的具体框架，以cifar-10上的图像分类任务为例

<!--more-->

***

## Get Started

### Tensors

Tensors are similar to NumPy’s ndarrays and use on a GPU to accelerate computing.

`torch.rand(5, 3)`在(0, 1)内随机初始化

`torch.zeros(a, b, dtype=torch.long)`用0初始化

`torch.tensor([5.5, 3])`将数据直接转为数组

`x = torch.randn_like(x, dtype=torch.float)`同size,也可ones,zeros...(dtype默认继承)

`x.size()`输出尺寸

### Operations

加法: `x+y` `torch.add(x, y)` `torch.add(x, y, out=result)` `y.add_(x) --a.x_(b)类操作会直接改变y`

变形: `x = x.view(a, b)` -1则自动推断

`x.item()`从tensor中获得python数

### Numpy Bridge

Tensor → NumPy Array: `a.numpy()`

NumPy Array → Tensor: `torch.from_numpy(a)`

### CUDA Tensors

用.to可以把数据放在任何device

```
# let us run this cell only if CUDA is available
# We will use ``torch.device`` objects to move tensors in and out of GPU
if torch.cuda.is_available():
    device = torch.device("cuda")          # a CUDA device object
    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU
    x = x.to(device)                       # or just use strings ``.to("cuda")``
    z = x + y
    print(z)
    print(z.to("cpu", torch.double))       # ``.to`` can also change dtype together!
    
Out:
tensor([2.9218], device='cuda:0')
tensor([2.9218], dtype=torch.float64)
```

***

## Autograd

### Tensor

用`torch.Tensor`是核心.用`a.requires_grad=True 或 x = torch.ones(2, 2, requires_grad=True)`开启(默认False),记录history用于backward

取消追踪history: `.detach()` 或 `with torch.no_grad(): *适用于评估模型时`

  `autograd.Function` - Implements forward and backward definitions of an autograd operation. Every `Tensor` operation, creates at least a single `Function` node, that connects to functions that created a `Tensor` and encodes its history.


### Gradients

对于scalar: `out.backward()`求导

gradients d(out)/dx: `x.grad`

  If any of tensors are non-scalar and require gradient, the function additionally requires specifying `grad_tensors` of matching length, which contains gradient of the differentiated function w.r.t. corresponding tensors (None is for tensors that don’t need gradient).

***

## Neural Networks

Neural networks can be constructed using the `torch.nn` package.

`nn` depends on autograd to define models and differentiate them. An `nn.Module` contains layers, and a method `forward(input)` that returns the output.

### Define the network
```
import torch
import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        # 1 input image channel, 6 output channels, 5x5 square convolution
        # kernel
        self.conv1 = nn.Conv2d(1, 6, 5)
        self.conv2 = nn.Conv2d(6, 16, 5)
        # an affine operation: y = Wx + b
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        # Max pooling over a (2, 2) window
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        # If the size is a square you can only specify a single number
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = x.view(-1, self.num_flat_features(x))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

    def num_flat_features(self, x):
        size = x.size()[1:]  # all dimensions except the batch dimension
        num_features = 1
        for s in size:
            num_features *= s
        return num_features


net = Net()
print(net)
```
You just have to define the forward function, and the backward function .is automatically defined 

get params: `params = list(net.parameters())`

    torch.nn only supports mini-batches
    If you have a single sample, just use input.unsqueeze(0) to add a fake batch dimension.

### Loss Function

```
output = net(input)
target = torch.randn(10)  # a dummy target, for example
target = target.view(1, -1)  # make it the same shape as output
criterion = nn.MSELoss()

loss = criterion(output, target)
print(loss)
```

### Backprop and Update the weights

```
import torch.optim as optim

# create your optimizer
optimizer = optim.SGD(net.parameters(), lr=0.01)

# in your training loop:
optimizer.zero_grad()              # Zero the gradient buffers
output = net(input)                # Forward
loss = criterion(output, target)   # Calculate loss
loss.backward()                    # Backprop
optimizer.step()                   # Does the update
```

***

## Training a Classifier

### Data

  Generally, when you have to deal with image, text, audio or video data, you can use standard python packages that load data into a numpy array. Then you can convert this array into a `torch.*Tensor`.

* For images, packages such as Pillow, OpenCV are useful
* For audio, packages such as scipy and librosa
* For text, either raw Python or Cython based loading, or NLTK and SpaCy are useful

  Specifically for vision, we have created a package called `torchvision`, that has data loaders for common datasets and data transformers for images--`torchvision.datasets` and `torch.utils.data.DataLoader`.

### Train an Image Classifier

#### 1. Loading and normalizing CIFAR10
Use `torchvision`
```
import torch
import torchvision
import torchvision.transforms as transforms

# The output of torchvision datasets are PILImage images of range [0, 1]. 
# We transform them to Tensors of normalized range [-1, 1].
transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,shuffle=True, num_workers=2)
testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)
```

#### 2. Define a Convolution Neural Network

```
import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


net = Net()


# 或快速搭建法
net = torch.nn.Sequential(
        torch.nn.xxx
        torch.nn.xxx)

```

#### 3. Define a Loss function and optimizer

```
import torch.optim as optim

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
```

#### 4. Train the network

```
for epoch in range(2):  # loop over the dataset multiple times

    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):  # enumerate用于将一个可遍历的数据对象组合为一个索引序列，同时列出数据和数据下标
        # get the inputs
        inputs, labels = data

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 2000 == 1999:    # print every 2000 mini-batches
            print('[%d, %5d] loss: %.3f' %
                  (epoch + 1, i + 1, running_loss / 2000))
            running_loss = 0.0

print('Finished Training')
```

#### 5. Test the network on the test data

```
# load data in mini-batches
dataiter = iter(testloader)
images, labels = dataiter.next()

# acc
correct = 0
total = 0
with torch.no_grad():
    for data in testloader:
        images, labels = data
        outputs = net(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print('Accuracy of the network on the 10000 test images: %d %%' % (
    100 * correct / total))
    
# 获得每一个class的acc
class_correct = list(0. for i in range(10))
class_total = list(0. for i in range(10))
with torch.no_grad():
    for data in testloader:
        images, labels = data
        outputs = net(images)
        _, predicted = torch.max(outputs, 1)      # torch.max(input, dim) return (max, max_indices)
        c = (predicted == labels).squeeze()       # .squeeze()--returns a tensor with all the 
                                                  #             dimensions of input of size 1 removed
        for i in range(4):
            label = labels[i]
            class_correct[label] += c[i].item()
            class_total[label] += 1


for i in range(10):
    print('Accuracy of %5s : %2d %%' % (
        classes[i], 100 * class_correct[i] / class_total[i]))
```

### Optimizer

SGD:mini batch

Momentum: 加一个速度

AdaGrad: 直走

RMSProp: 结合Momentum和AdaGrad

Adam: 进一步结合

### Save and Load Net

保存:

`torch.save(net, 'name')                # 整个网络`
`torch.save(net.state_dict(), 'name')   # 参数`  

读取:
`net = torch.load('name')   # 整个网络`
`先将原net结构复原...  net.load_state_dict(torch.load('name'))  # 参数`

### Train on GPU

define our device:
`device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")`

send the whole net as well as inputs and outputs in every step to gpu
```
net.to(device)
inputs, labels = inputs.to(device), labels.to(device)
```
***

## Data process

### Customize Dataset

自己定义的数据集需要继承抽象类`torch.utils.data.Dataset`,重写`__init__`:基本参数, `__getitem__`:通过索引来读取数据, `__len__`:整个数据集的尺寸大小

```
class MyDataset(torch.utils.data.Dataset):       # 需要继承torch.utils.data.Dataset

    def __init__(self):
        # 1. 初始化文件路径，并将文件名和标签值放入一个列表Initialize file path or list of file names.
	      # 2. 初始化自身参数 
        pass
        
    def __getitem__(self, index):
        # 1. 从自身数据列表中读取一个数据Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).
        # 2. 使用自身的transform处理数据Preprocess the data (e.g. torchvision.Transform).
        # 3. 返回一个数据和其对应的标签Return a data pair (e.g. image and label).
        # 这里需要注意的是，第一步：read one data，是一个data
        pass
        
    def __len__(self):
        # 返回数据集合的大小You should change 0 to the total size of your dataset.
        return len
```

### Data Augmentation











