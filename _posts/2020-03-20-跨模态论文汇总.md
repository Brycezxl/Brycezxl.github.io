---

title: 跨模态论文汇总
tags:
 - Cross-Modal
 - GNN
 - NLP
 - CV
---



<!--more-->

## **TSG**

Temporal Sentence Grounding



----



### **17 MCN**

Moment Context Network



**motivation**

1. 使用自然语言可以检索目标的复杂动作
2. 现有方法只能检索整个视频是否有某个动作，而不能确定动作出现的具体时刻
3. 现有方法忽略了视频的全局信息，而全局信息也是定位的关键线索
4. 提出数据集DiDeMo



**模型**

![img](/pictures/Cross-Modal/MCN.png)            

对于文本内容，用lstm提取，需要人工处理的视频片段

视频内容分成三个部分：

1. 全局信息，整个视频cnn的平均池化
2. 当前信息，片段cnn的平均池化
3. 时间信息，归一化到【0,1】的片段结束时间

同时输入视频也有两种：RGB和光流差

差值D = (RGB - lstm)^2 + k * (光流差 - lstm)

Loss分为两部分，第一部分比较本视频的D1与其他视频的D2，要求D2>D1，第二部分是比较本视频中正确段落的D1与其他段落的D3，要求D3>D1



**想法**

1. 定位时，给视频人工划分段落再判断，人力复杂且不准确
2. 获得全局信息时，对整段视频使用cnn再池化，只能识别到平均画面，不能有效提取全局信息，可以使用attention等



------





### **17 TALL**

 Temporal Activity Localization 



**motivation**

1. 现有方法只能识别固定的一些动作，因此需要使用自然语言
2. 视频分段复杂，使用bounding box回归的方法找到视频具体边界



**模型**

 ![img](/pictures/Cross-Modal/TALL.png)            

分四个部分：

1. 视频处理：对于每个视频切片，对每一个window用卷积提取中间内容，卷积+池化分别提取前+后的内容，变成向量

2. 文字处理：lstm

3. 跨模态处理：按位加，按位乘，FC，之后拼接

4. 回归定位：输出两个值，第一个是视频和文字的匹配值，第二个是视频剪裁值，对应两个Loss



**想法**

1. 定位动作时，使用window内加偏移的方法确定视频的边界避免了人工划分，但是需要计算不同的帧、window大小对应的视频，费时、在训练时容易重叠且对片段长度有要求
2. 处理视频时使用卷积+池化处理了前后的内容，也算是视频的小全局信息，比MCN更有效，但是也感觉太粗糙了



------



### **18 FIFO**

Find and Focus



**motivation**

1. 之前的研究都把视频表示成一个特征向量，通过特征匹配来完成检索。这存在两个问题：a.单个特征向量不能表示出视频的结构；b.特性向量不能定位（胡说
2. 也把自然语言段落（之前的研究好像都是句子）表示成一个向量，不能捕捉句与句关系，也不能有效利用单句信息
3. 基本想法：深入视频和文字的内部信息，不仅建立段落vs视频、更构建句子vs片段的关系



**模型**

首先用cnn对视频的每个片段（6帧为一段）和段落的每句话用cnn编码

再根据每个片段和句子的相似度找出多个与句子有关的视频切片

通过最大化切片和和句子的相似度，使得每个句子找到最多k个配对的视频切片，每个视频切片只对应一个句子



**想法**

1. 视频分段的方法值得参考，对于单句检索也可以从粗到细
2. 这篇文章研究是段落定位，但是对于单句语言也可以利用不同的句子成分
3. 特征处理的过于简单，最后匹配的准确度也不高



------



### **18 TMN**

Temporal Modular Networks



**motivation**

1. 现有研究没有推理
2. 现有的rnn和cnn方法无法理解视频结构与句子的成分结构
3. 基本想法：句子成分结构对理解视频并定位非常重要



**模型**

 ![img](/pictures/Cross-Modal/TMN.png)            

需要预先切片，先用parser将句子变成语法树，将无用的标签舍弃，合并相似的标签，最终剩下8个标签

对于语法树树叶使用词与视频的attention+全连接，对于树枝使用attention融合子节点的内容，最后与视频内容比较



**想法**

1. 语法树推理的想法能一定程度解决理解问题
2. 更多算是信息结合，不能能算的上是推理（可以叠加多个网络来完成推理？
3. 语法树内没有很好结合视频内容，叶节点信息模糊



------



### **18 MLLC**

Moment Localization with Latent Context



**motivation**

1. 之前的研究会考虑与query无关的上下文内容



**模型**

![img](/pictures/Cross-Modal/MLLC.png)            

通过语言和片段的相似度挑选context

视频提取与mcn一样，视频内容中又加入了context的部分，对RGB和光流差使用cnn+池化

片段起始特征用片段在原来视频、context视频中的开始、结束时间，共4个

语言用lstm

相似度和边界，用TALL中的对视频和语言向量做拼接、相加、点乘



**想法**

1. motivation挺好，但做的挺水的，主要是加上了context的内容，本质上是增加了context内容在池化时所占的比例，并没有太多的创新，只能一定程度上解决这个问题
2. 使用attention忽略掉比较无关的内容比较好，对内容和视频的处理也比较浅



------



### **18 TGN**

Temporal GroundNet



**motivation**

1. 现有研究把视频和语句变成向量来比较，不够expressive，忽略了单词和视频片段的关系（之前有一个用语法树分析语句的
2. 需要重复多窗口查找，计算量大



**模型**

![img](/pictures/Cross-Modal/TGN.png)            

encoder：用两个lstm获取视频和语句的所有隐藏层

interactor：先获得每一帧对所有语句的attention求和，将其与视频向量拼接，再通过一个lstm输出最后的向量

grounder：输出当前时刻结束的，长度不同的视频（比如2秒，4秒，6秒前开始的）的分数，到整个视频结束后选最高的



**想法**

1. attention能让每一帧结合对应的语句，但是只由lstm传递上下文信息，不够
2. 无关的视频片段可能会影响lstm中信息质量
3. grounder的想法挺好，就是时间差固定位不准确，是不是可以加一个回归
4. 相比于语法树，结合了更细的语言特征但是失去了语言结构



------



### **18 ACRN**

Attentive Cross-Modal Retrieval Network



**motivation**

1. 视频片段有不同的时长、时序结构，用window的话范围不精确且计算量大
2. 之前的工作都把视频和语句嵌入到高维空间判断相似度，但是会忽略很多时序细节



**模型**

![img](/pictures/Cross-Modal/ACRN.png)            

处理视频时，根据视频当前以及前后一小段时间前的帧对语句attention，再将这些帧数求和得到当前部分的值。

将视频、语句、视频、点乘拼接，输出偏移量与匹配值



**想法**

1. 是TALL的改进版，和grounder定位思路有点像，但是没它做的好
2. 最后的池化还是忽略了很多信息
3. 只有attention，没有视频结构的推理，对时序逻辑把握的不好
4. 对语句还是只用一个向量，忽略细节



------



### **19 QSPN**

Query-Guided Segment Proposal Network



**motivation**

1. 单个向量不能表达视频和语句里的结构信息
2. 以前研究里视频和语言的embedding是独立的，不能直接操作比较
3. 主要还是需要更好地抓住结构信息



**模型**

 ![img](/pictures/Cross-Modal/QSPN.jpeg)            

对于一个视频切片，通过attention得到视频每一帧的分数，在lstm时使用，并筛去一部分不相关的片段

有两层lstm，第一层依次输入query的所有词，第二层输入权重化的每一帧，最后输出分数与语言还原



**想法**

1. attention已经有人做过了
2. 还是需要人工把视频切片
3. 最后依次输入lstm不能算是真的理解结构，而且也有人做过
4. 语句复原算是亮点，但是通过这么简单的操作感觉不大能精确复原
5. 可否用GAN进行复原操作？



------



### **19 Read**

Read, Watch and Move 



**motivation**

1. 之前的研究使用window或者列出所有可能的切片，太过复杂
2. 基本想法：给定描述和当前视频片段的开始结束时间，人看完视频能将开始结束点调整到更合适的点，不断迭代直到找到最优片段



**模型**

![img](/pictures/Cross-Modal/Read.png)            

观察：有四个要素---语句特征，全局特征，本地特征，视频位置特征，通过FC处理

actor-critic：将上述观察输入GRU，输出当前动作（左、右边界怎么移动）和当前状态评分

mutitask：根据边界和tIoU（归一化边界）进行回归



**想法**

1. motivation和18年的两篇有些重复，前两篇虽然不是用window，但都需要每一步都找很多candidate，这篇避免了这个问题，重复迭代的解决方法很新颖
2. 对语言和视频的分析需要再深入
3. 对目标片段长度有局限



------



### **19 SAP**

Semantic Activity Proposal



**motivation**

1. 之前的研究candaidates很多，计算量大
2. 之前的研究忽视了静态的场景



**模型**

​            ![img](/pictures/Cross-Modal/SAP.png)            

视觉元素探测：视觉元素主要指物体和动作，假定描述对应的视频片段每一帧都涵盖描述中的所有物体和动作，那么给定一帧，预测描述中的物体和动作，当一个分类任务来训练即可

动作提取：根据上一步得到了每一帧的表达，和描述相乘获得相似度，高于阀值的算positive，再用某算法将positive帧聚类，得到多个长度相同可重叠的proposal

精调：结合句子、cnn、视觉元素，通过FC输出边界偏移量和最后分数



**想法**

1. 除了正常的cnn，还用一个额外的cnn提取视觉元素，一来初步筛去了很多无关片段，二来掌握了画面中更精细更符合描述的细节特征，可以说是之前几个attention研究的升级版
2. 忽略了动作之间的连贯性逻辑性
3. 定长的聚类可能效果不好，也有可能一段视频中间有几帧没过阀值而导致这段视频被拆开，也就找不到正确答案了
4. 训练视觉元素探测可能效果不好，不如之间用现成的模型
5. 逻辑结构没有
6. 可不可以根据句子内容或者句子中视觉元素生成cnn？



------



### **19 ABLR**

Attention Based Location Regression



**motivation**

1. 之前用的都是scan and localize的方法，有三个缺点
2. 片段失去了时序和全局的信息
3. 用一个向量表示视频和文字会缺失很多细节信息
4. 计算量大



**模型**

![img](/pictures/Cross-Modal/ABLR.png)            

想法----要找到视频和文字的重点，所以有了3次attention：文字attention平均池化的视频内容，粗略获得文字中的主要内容A1；视频内容与A1 attention，获得和文字相关的视频段落，初始文字与A2 attention，获得精细的重要的文字内容A3

根据A2和A3直接预测边界



**想法**

1. 算是目前为止比较好的attention利用，同时关注到了视频和文本
2. 还是少了一些逻辑和结构，对first、then这种情况考虑的比较少
3. 视频和文本中加上全局信息（self-attention）是不是比较好？



------



### **19 PFGA**

Proposal-free Temporal Moment Localization of a Natural-Language Query in Video using Guided Attention



**motivation**

1. 之前的研究都是基于candidate ranking或者generation，需要人工定义片段（并不
2. 现在要直接预测开头和结尾的时间点



**模型**

​            ![img](/pictures/Cross-Modal/PFGA.png)            

I3D处理视频，GRU+FC处理文本

视频对文本guided attention再经过GRU得到开始和结束的时间

guided attention是对attention加loss，要求正确帧数的attention更大



**想法**

1. attention非常简单但新加一个loss的想法很新颖，效果也很好
2. 为什么不做一个self-attention？可以重复两次attention来推理，可以解决时空和空间错位？



------



###  **19 TripNet**

Tripping through time



**motivation**

1. 主要思想是提速
2. 不需要看视频的所有部分，更不用看多次视频



**模型**

​            ![img](/pictures/Cross-Modal/TripNet.png)            

也是actor-critic，是Read，Watch and Move的简化版

根据当前片段和文本的attention，输出行为的评分，输出的行为是左右边界都加上j，视频片段长度始终保持不变



**想法**

1. 快就完事了？



------



### **19 MAN**

Moment Alignment Network



**motivation**

1. 之前的研究都是用window或者将文本与视频逐帧比较，缺少了逻辑上的理解会有semantic misalignment（second time）和structural misalignment（after）



**模型**

![img](/pictures/Cross-Modal/MAN.png)            

视频需要先切片

编码：将视频和文本处理以后通过动态卷积对齐，解决了语义错位。借鉴了SSD的想法，逐步池化卷积，获得时间粒度越来越大的特征，这样每一个向量就对应了不同的时间的和长度

IGAN：用GCN和RNN根据刚刚获得的向量进行推理，解决结构错位，获得最后的分数



**想法**

1. motivation挺好，但是时间对齐的操作很诡异
2. 用图推理挺好，但是最好结合更多的画面信息
3. 是否可以换一种推理方式，类似于QA里的？
4. 其他目标检测里的算法像solo能不能借鉴？



------



### **19 SMRL**

semantic matching reinforcement learning



**motivation**

1. 之前的工作用window
2. 之前对全局用池化，失去了时序信息



**模型**

![img](/pictures/Cross-Modal/SMRL.png)            

视频：用一个cnn提取某帧的semantic concept（物品、动作），加一个loss训练，再结合全局特征、位置信息，送入lstm中，再输出结果

文本：skip thoughts

FC结合两者输出下一个要看的帧、loss、reward



**想法**

1. 结果很差，或许是因为看的帧太少了比较片面，而且全局信息没提取好，导致下一帧选择的不准确
2. 和read，watch and move思路很像，但是加了个语义概念提取，但是最后的action还是不如那篇的移动边界好



------



### **19 DEBUG**

DEnse Bottom-Up Grounding



**motivation**

1. 之前预测boundary的研究存在三个问题
2. 2个边界的预测是独立的，忽略了中间内容的独立性
3. 正反例子不平衡
4. 根据帧来预测动作边界很难



**模型**

 ![img](/pictures/Cross-Modal/DEBUG.png)            

分类loss解决不平衡问题，回归loss解决缺失中间语义问题，多loss缓解预测难的问题



**想法**

1. 主要是套了个transformer的模型，再加了多个loss
2. 不一定会忽略中间信息，因为是根据所有信息来得出boundary的
3. 每一帧都分类确实能解决不平衡问题，但是失去了联系还复杂



------



### **19 SLTA**

Spatial and Language-Temporal Attention



**motivation**

1. 之前的研究不能辨认出相关物体和互动
2. 不能很好理解query中的关键词语



**模型**

![img](/pictures/Cross-Modal/SLTA.png)            



**想法**

1. 类似的研究也研究了识别画面中的物体，不过都只和cnn提取出的特征结合，这里将物体与query相结合，一定程度上找到了他们的互动
2. 互动部分只用lstm链接感觉语义不强，可以再加一些周围部分的特征
3. 失去了结构特征，会出现逻辑错位，可以在互动部分也加上



------



### **19 ExCL**

Extractive Clip Localization



**motivation**

1. 基于window的计算量大，受window范围影响
2. 基于相似度排序的太复杂，难训练



**模型**

![img](/pictures/Cross-Modal/ExCL.png)            

文本：双向LSTM取最后一个向量

视频：I3D+双向LSTM取每一个hidden state

两者拼接，通过一个LSTM输出当前帧作为开始、结尾的分数，对分数回归/二分类



**想法**

1. 很简单的操作，但是结果却非常好
2. 可能是因为它计算loss的方式不同，别的都是结合所有帧直接预测boundary，他是为每一帧都预测一个分数，更细？
3. 说明lstm已经能够处理部分结构信息？



------



### 19 SCDM

semantic conditioned dynamic modulation



**motivation**

1. 现有研究主要关注语义上匹配文本和视频信息，却忽略了文本能够帮助关联和定位视频片段
2. 视频中的动作表现、时长不一，文本也需要在不同时间维度上动态结合多种动作



**模型**

​            ![img](/pictures/Cross-Modal/SCDM.png)            

核心思想：输出不同时间维度的特征来理解不同长度的动作；同时利用文本信息，通过定制卷积来更好地关联、定位动作

SCDM：先根据代表某一时间范围的特征向量与文本信息attention，获得当前较为重要的词组合成的特征向量，再根据这个向量对视频向量生成一个动态的线性变换，可以说在这个时间段内强调、定位了某动作，最后输出新的向量，再不断池化使其获得关联



**想法**

1. 大致思路和MAN很像，优点在于加入了SCDM，使每一步中都融入了文本的信息，而且效果提升了很多，说明文本信息很重要
2. 这样的方法有点在于能考虑到不同时间维度上的信息，且在卷积时使相邻信息间得到了互动
3. 缺点在于，噪声很大，训练时会把很多的不相关信息都包含进去，且卷积没有考虑到视频的时序



------



### **19 CMIN**

Cross-Modal Interaction Network



**motivation**

1. 之前的研究都只关注一个方面，像是query表示、视频建模、跨模态融合，没有一个全面的系统
2. 这个研究全面关注了这三个问题：query的语法结构、视频中长距离的语义依赖、跨模态融合



**模型**

 ![img](/pictures/Cross-Modal/CMIN.png)            

在GCN中用GRU获得context，为不同语法关系设置不同参数，不断更新q

然后得出每一帧的信息，根据window给分数



**想法**

1. 用图结构探索了语义结构，这个想法感觉不错
2. 视频没有很好结合语义



------



### **19 ACL**

Activity Concepts based Localizer



**motivation**

1. 没有注意到视频中与文本相关的语义内容
2. 没有注意到文本中的语法对
3. 用window时没有挑选



**模型**

 ![img](/pictures/Cross-Modal/ACL.png)            

vo是verb-obj，即动作-目标，视频中的visual activity预测它

actionness score代表窗口中含有目标片段的可能



**想法**

1. 和之前几篇有些像，大致结构都差不多
2. vo这个不好预测，而且label太多，不如直接预测物品或者动作



------



### **20 CBP**

Contextual Boundary-aware Prediction



**motivation**

1. 基于archor的方法有重叠不能精确定位，基于边界的方法有时会定位错误



**模型**

![img](/pictures/Cross-Modal/CBP.png)            



**想法**

1. 主要就结合了两个loss



------



### **20 2D-TAN**

2D map Temporal Adjacent Network



**motivation**

1. 现有研究分开考虑每一个candidate，忽略了不同candidate之间的关系，也就不能分别相关candidate之间细微的语义差别，导致定位不准确



**模型**

![img](/pictures/Cross-Modal/2D-TAN.png) 



**想法**

1. 2D时间图的想法很好，抓住了不同时间片段之间的细微差距

-----

